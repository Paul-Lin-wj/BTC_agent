# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨ç›‘æ§æ™ºèƒ½ä½“ç³»ç»Ÿ - é›†æˆç‰ˆ
æ•´åˆäº†æ•°æ®è·å–ç®¡ç†ã€æ•°æ®åˆ†æã€å›¾è¡¨ç”Ÿæˆã€BTCå®æ—¶ä»·æ ¼æŸ¥è¯¢ç­‰åŠŸèƒ½
"""

import os
import sys
import json
import asyncio
import csv
import subprocess
import signal
import websocket
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

# å…³é—­ loguru çš„ DEBUG æ—¥å¿—
from loguru import logger as loguru_logger
loguru_logger.remove()
loguru_logger.add(lambda _: None, level="WARNING")
loguru_logger.add(sys.stderr, level="WARNING")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/data/juno/lin/agent/drsai-main')
sys.path.insert(0, '/data/juno/lin/agent/drsai-main/my_agent/stock_monitor/python')

from drsai import AssistantAgent, HepAIChatCompletionClient
from drsai.backend import run_worker, run_console
from drsai.modules.managers.database import DatabaseManager
from drsai import tools_recycle_reply_function

import finnhub

# ==================== é…ç½® ====================
DEBUG_MODE = False  # True=å‘½ä»¤è¡Œæ¨¡å¼, False=åç«¯APIæœåŠ¡æ¨¡å¼

# CSVæ–‡ä»¶è·¯å¾„
CSV_FILE = "/data/juno/lin/agent/drsai-main/my_agent/stock_monitor/data/stock_data.csv"

# å›¾è¡¨ä¿å­˜ç›®å½•
CHART_DIR = "/data/juno/lin/agent/drsai-main/my_agent/stock_monitor/charts"
os.makedirs(CHART_DIR, exist_ok=True)

# data_get.py è„šæœ¬è·¯å¾„
DATA_GET_SCRIPT = "/data/juno/lin/agent/drsai-main/my_agent/stock_monitor/python/data_get.py"
PYTHON_EXECUTABLE = "/datafs/users/lin/python-venv/drsai/bin/python"

# ==================== Matplotlibé…ç½® ====================
# è®¾ç½®matplotlibä½¿ç”¨éGUIåç«¯
matplotlib.use('Agg')

# é…ç½®ä¸­æ–‡å­—ä½“
import matplotlib.font_manager as fm
available_fonts = set([f.name for f in fm.fontManager.ttflist])

preferred_fonts = [
    'Noto Sans CJK SC', 'Noto Sans CJK JP', 'Noto Serif CJK JP',
    'WenQuanYi Zen Hei Sharp', 'AR PL UMing CN', 'SimSun', 'SimHei',
]

selected_font = None
for font in preferred_fonts:
    if font in available_fonts:
        selected_font = font
        break

if selected_font:
    plt.rcParams['font.sans-serif'] = [selected_font, 'DejaVu Sans']
else:
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']

plt.rcParams['axes.unicode_minus'] = False

# ==================== æ•°æ®å¤„ç†è¾…åŠ©å‡½æ•° ====================

def _apply_time_filter_df(df: pd.DataFrame, time_filter: str = None) -> pd.DataFrame:
    """åº”ç”¨æ—¶é—´è¿‡æ»¤åˆ°DataFrame"""
    if not time_filter:
        return df

    import re
    now = datetime.now()
    time_filter_original = time_filter.strip()
    time_filter = time_filter_original.lower()

    # æŒ‰åˆ†é’Ÿè¿‡æ»¤
    if 'min' in time_filter or 'åˆ†é’Ÿ' in time_filter:
        match = re.search(r'(\d+)\s*(min|åˆ†é’Ÿ)', time_filter)
        if match:
            mins = int(match.group(1))
            start = now - timedelta(minutes=mins)
            return df[(df['datetime'] >= start) & (df['datetime'] <= now)].copy()
        cn_min_map = {'å…­å': 60, 'äº”å': 50, 'å››å': 40, 'ä¸‰å': 30,
                      'äºŒå': 20, 'åäº”': 15, 'åäºŒ': 12, 'åä¸€': 11,
                      'å': 10, 'ä¹': 9, 'å…«': 8, 'ä¸ƒ': 7, 'å…­': 6,
                      'äº”': 5, 'å››': 4, 'ä¸‰': 3, 'äºŒ': 2, 'ä¸¤': 2, 'ä¸€': 1}
        for cn_num, mins in cn_min_map.items():
            if cn_num in time_filter and 'åˆ†é’Ÿ' in time_filter:
                start = now - timedelta(minutes=mins)
                return df[(df['datetime'] >= start) & (df['datetime'] <= now)].copy()

    # æŒ‰å°æ—¶è¿‡æ»¤
    hour_patterns = [
        ('è¿‡å»ä¸€å°æ—¶', '1h', '1å°æ—¶', 1),
        ('2h', '2å°æ—¶', 'ä¸¤å°æ—¶', 2),
        ('3h', '3å°æ—¶', 'ä¸‰å°æ—¶', 3),
        ('6h', '6å°æ—¶', 'å…­å°æ—¶', 6),
        ('12h', '12å°æ—¶', 'åäºŒå°æ—¶', 12),
        ('24h', '24å°æ—¶', 'äºŒåå››å°æ—¶', 24),
    ]
    for pattern in hour_patterns:
        if any(p in time_filter for p in pattern[:-1]):
            start = now - timedelta(hours=pattern[-1])
            return df[(df['datetime'] >= start) & (df['datetime'] <= now)].copy()

    # æŒ‰å¤©è¿‡æ»¤
    if any(kw in time_filter for kw in ['ä»Šå¤©', 'ä»Šæ—¥']):
        start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        filtered = df[(df['datetime'] >= start) & (df['datetime'] <= now)].copy()
        if filtered.empty:
            start = now - timedelta(hours=24)
            filtered = df[(df['datetime'] >= start) & (df['datetime'] <= now)].copy()
        return filtered
    elif 'æ˜¨å¤©' in time_filter and 'ç‚¹' not in time_filter:
        yesterday = now - timedelta(days=1)
        start = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
        end = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999)
        return df[(df['datetime'] >= start) & (df['datetime'] <= end)].copy()

    # å…·ä½“æ—¶é—´æ®µè§£æ
    cn_hour_map = {
        'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸¤': 2, 'ä¸‰': 3, 'å››': 4,
        'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
        'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15,
        'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20,
        'äºŒåä¸€': 21, 'äºŒåäºŒ': 22, 'äºŒåä¸‰': 23
    }

    day_offset = 0
    if 'æ˜¨å¤©' in time_filter:
        day_offset = -1
    elif 'å‰å¤©' in time_filter:
        day_offset = -2
    elif 'æ˜å¤©' in time_filter:
        day_offset = 1

    is_afternoon = 'ä¸‹åˆ' in time_filter
    is_morning = 'ä¸Šåˆ' in time_filter or 'å‡Œæ™¨' in time_filter or 'æ—©ä¸Š' in time_filter

    # åŒ¹é…é˜¿æ‹‰ä¼¯æ•°å­—å°æ—¶èŒƒå›´
    range_match = re.search(r'(\d{1,2})\s*ç‚¹.*?(\d{1,2})\s*ç‚¹', time_filter)
    if range_match:
        h1 = int(range_match.group(1))
        h2 = int(range_match.group(2))
        if is_afternoon and h1 < 12:
            h1 += 12
        if is_afternoon and h2 < 12:
            h2 += 12
        base_date = (now + timedelta(days=day_offset)).replace(hour=0, minute=0, second=0, microsecond=0)
        start = base_date + timedelta(hours=h1)
        end = base_date + timedelta(hours=h2)
        return df[(df['datetime'] >= start) & (df['datetime'] <= end)].copy()

    # åŒ¹é…å•ç‚¹å°æ—¶
    hour_match = re.search(r'(\d{1,2})\s*ç‚¹[^åˆ°è‡³]', time_filter)
    if not hour_match:
        hour_match = re.search(r'(\d{1,2})\s*ç‚¹$', time_filter)
    if hour_match:
        hour = int(hour_match.group(1))
        if is_afternoon and hour < 12:
            hour += 12
        elif is_morning and hour == 12:
            hour = 0
        base_date = (now + timedelta(days=day_offset)).replace(hour=0, minute=0, second=0, microsecond=0)
        start = base_date + timedelta(hours=hour)
        end = base_date + timedelta(hours=hour+1)
        return df[(df['datetime'] >= start) & (df['datetime'] <= end)].copy()

    return df


# ==================== å·¥å…·å‡½æ•°ï¼šæ•°æ®è·å–ç®¡ç† ====================

def check_data_quality(check_count: int = 10) -> str:
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    try:
        if not os.path.exists(CSV_FILE):
            return f"## æ•°æ®è´¨é‡æ£€æŸ¥\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"

        with open(CSV_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            rows = list(reader)

        if not rows:
            return "## æ•°æ®è´¨é‡æ£€æŸ¥\nâŒ æ•°æ®æ–‡ä»¶ä¸ºç©º"

        latest_rows = rows[-check_count:] if len(rows) >= check_count else rows
        zero_count = sum(1 for r in latest_rows if float(r.get('price', 0)) == 0)
        valid_count = len(latest_rows) - zero_count

        latest_time = latest_rows[-1].get('datetime', 'Unknown')
        latest_price = float(latest_rows[-1].get('price', 0))

        if zero_count == len(latest_rows):
            return f"""## æ•°æ®è´¨é‡æ£€æŸ¥

**çŠ¶æ€:** âš ï¸ å¼‚å¸¸
**é—®é¢˜:** æœ€æ–°æ•°æ®å…¨æ˜¯å ä½æ•°æ®ï¼ˆprice=0ï¼‰
**æ£€æŸ¥æ•°é‡:** {len(latest_rows)}æ¡
**æœ‰æ•ˆæ•°æ®:** 0æ¡

**å»ºè®®:** è¯·é‡å¯æ•°æ®è·å–æœåŠ¡

æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        elif zero_count > len(latest_rows) // 2:
            return f"""## æ•°æ®è´¨é‡æ£€æŸ¥

**çŠ¶æ€:** âš ï¸ éƒ¨åˆ†å¼‚å¸¸
**é—®é¢˜:** æœ€æ–°{len(latest_rows)}æ¡ä¸­æœ‰{zero_count}æ¡å ä½æ•°æ®
**æœ‰æ•ˆæ•°æ®:** {valid_count}/{len(latest_rows)}æ¡
**æœ€æ–°ä»·æ ¼:** {latest_price:.2f} USDT

**å»ºè®®:** è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥

æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        else:
            return f"""## æ•°æ®è´¨é‡æ£€æŸ¥

**çŠ¶æ€:** âœ… æ­£å¸¸
**æ£€æŸ¥æ•°é‡:** {len(latest_rows)}æ¡
**æœ‰æ•ˆæ•°æ®:** {valid_count}æ¡
**æœ€æ–°ä»·æ ¼:** {latest_price:.2f} USDT
**æœ€æ–°æ—¶é—´:** {latest_time}

æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## æ•°æ®è´¨é‡æ£€æŸ¥\næ£€æŸ¥å¤±è´¥: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def get_data_collection_status(check_count: int = 10) -> str:
    """è·å–æ•°æ®è·å–æœåŠ¡çŠ¶æ€"""
    try:
        # å…ˆæ£€æŸ¥è¿›ç¨‹çŠ¶æ€
        result = subprocess.run(["pgrep", "-f", "data_get.py"], capture_output=True, text=True, timeout=5)

        if result.returncode == 0:
            running_pids = result.stdout.strip().split('\n')

            # ç®€åŒ–æ•°æ®è´¨é‡æ£€æŸ¥ - åªæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œæœ€åä¿®æ”¹æ—¶é—´
            quality_msg = ""
            try:
                if os.path.exists(CSV_FILE):
                    mtime = os.path.getmtime(CSV_FILE)
                    mod_time = datetime.fromtimestamp(mtime)
                    time_diff = (datetime.now() - mod_time).total_seconds()

                    # è¯»å–æœ€åä¸€è¡Œè·å–ä»·æ ¼
                    with open(CSV_FILE, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    if len(lines) > 1:
                        last_line = lines[-1].strip()
                        parts = last_line.split(',')
                        if len(parts) >= 2:
                            try:
                                latest_price = float(parts[1])
                                latest_dt = parts[0]
                                quality_msg = f"\n**æœ€æ–°ä»·æ ¼:** {latest_price:.2f} USDT\n**æ›´æ–°æ—¶é—´:** {latest_dt}\n**æ•°æ®æ›´æ–°:** {int(time_diff)}ç§’å‰"
                            except:
                                quality_msg = f"\n**æ•°æ®æ›´æ–°:** {int(time_diff)}ç§’å‰"
                        else:
                            quality_msg = f"\n**æ•°æ®æ›´æ–°:** {int(time_diff)}ç§’å‰"
                    else:
                        quality_msg = "\n**æ•°æ®è´¨é‡:** æ–‡ä»¶ä¸ºç©º"
                else:
                    quality_msg = "\n**æ•°æ®è´¨é‡:** æ–‡ä»¶ä¸å­˜åœ¨"
            except Exception as e:
                quality_msg = f"\n**æ•°æ®è´¨é‡:** æ£€æŸ¥å¤±è´¥"

            return f"""## æ•°æ®è·å–æœåŠ¡çŠ¶æ€

**çŠ¶æ€:** ğŸŸ¢ è¿è¡Œä¸­
**è¿›ç¨‹ID:** {', '.join(running_pids)}{quality_msg}

æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        else:
            return f"""## æ•°æ®è·å–æœåŠ¡çŠ¶æ€

**çŠ¶æ€:** ğŸ”´ æœªè¿è¡Œ

æç¤ºï¼šä½¿ç”¨"å¯åŠ¨æ•°æ®è·å–æœåŠ¡"å‘½ä»¤å¯åŠ¨

æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except subprocess.TimeoutExpired:
        return f"## æ•°æ®è·å–æœåŠ¡çŠ¶æ€\nâŒ æŸ¥è¯¢è¶…æ—¶\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    except Exception as e:
        return f"## æ•°æ®è·å–æœåŠ¡çŠ¶æ€\nâŒ æŸ¥è¯¢å¤±è´¥: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def start_data_collection(duration: int = 0) -> str:
    """å¯åŠ¨æ•°æ®è·å–æœåŠ¡"""
    try:
        if not os.path.exists(DATA_GET_SCRIPT):
            return f"## å¯åŠ¨æ•°æ®è·å–æœåŠ¡\nå¤±è´¥ï¼šè„šæœ¬ä¸å­˜åœ¨ {DATA_GET_SCRIPT}"

        # æ£€æŸ¥æ˜¯å¦å·²è¿è¡Œ
        existing = subprocess.run(["pgrep", "-f", "data_get.py"], capture_output=True, text=True)
        if existing.returncode == 0:
            pids = existing.stdout.strip().split('\n')
            return f"""## å¯åŠ¨æ•°æ®è·å–æœåŠ¡

**çŠ¶æ€:** å·²åœ¨è¿è¡Œä¸­
**è¿›ç¨‹ID:** {', '.join(pids)}

æ— éœ€é‡å¤å¯åŠ¨

æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""

        output_file = "/data/juno/lin/agent/drsai-main/my_agent/stock_monitor/python/nohup.out"
        cmd = f"nohup {PYTHON_EXECUTABLE} {DATA_GET_SCRIPT} > {output_file} 2>&1 &"
        subprocess.run(cmd, shell=True, check=True)

        import time
        time.sleep(2)

        pid_result = subprocess.run(["pgrep", "-f", "data_get.py"], capture_output=True, text=True)
        if pid_result.returncode == 0:
            pids = pid_result.stdout.strip().split('\n')
            latest_pid = pids[-1] if pids else "Unknown"
        else:
            latest_pid = "Unknown"

        return f"""## å¯åŠ¨æ•°æ®è·å–æœåŠ¡

**çŠ¶æ€:** âœ… å¯åŠ¨æˆåŠŸ
**è¿›ç¨‹ID:** {latest_pid}
**å¯åŠ¨æ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## å¯åŠ¨æ•°æ®è·å–æœåŠ¡\nå¤±è´¥: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def stop_data_collection() -> str:
    """åœæ­¢æ•°æ®è·å–æœåŠ¡"""
    try:
        result = subprocess.run(["pgrep", "-f", "data_get.py"], capture_output=True, text=True)
        if result.returncode == 0:
            pids = result.stdout.strip().split('\n')
            for pid in pids:
                os.kill(int(pid), signal.SIGTERM)
            return f"""## åœæ­¢æ•°æ®è·å–æœåŠ¡

**çŠ¶æ€:** âœ… å·²åœæ­¢
**å·²åœæ­¢è¿›ç¨‹:** {', '.join(pids)}

æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        else:
            return f"## åœæ­¢æ•°æ®è·å–æœåŠ¡\nçŠ¶æ€: æœåŠ¡æœªåœ¨è¿è¡Œ\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    except Exception as e:
        return f"## åœæ­¢æ•°æ®è·å–æœåŠ¡\nå¤±è´¥: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def restart_data_collection() -> str:
    """é‡å¯æ•°æ®è·å–æœåŠ¡"""
    # å…ˆåœæ­¢
    stop_info = ""
    try:
        result = subprocess.run(["pgrep", "-f", "data_get.py"], capture_output=True, text=True)
        if result.returncode == 0:
            pids = result.stdout.strip().split('\n')
            for pid in pids:
                os.kill(int(pid), signal.SIGTERM)
            stop_info = f"å·²åœæ­¢è¿›ç¨‹: {', '.join(pids)}"
    except:
        stop_info = "åœæ­¢å¤±è´¥æˆ–æ— è¿›ç¨‹"

    import time
    time.sleep(1)

    # å†å¯åŠ¨
    try:
        if not os.path.exists(DATA_GET_SCRIPT):
            return f"## é‡å¯æ•°æ®è·å–æœåŠ¡\nå¤±è´¥ï¼šè„šæœ¬ä¸å­˜åœ¨"

        cmd = f"nohup {PYTHON_EXECUTABLE} {DATA_GET_SCRIPT} > /dev/null 2>&1 &"
        subprocess.run(cmd, shell=True, check=True)
        time.sleep(2)

        pid_result = subprocess.run(["pgrep", "-f", "data_get.py"], capture_output=True, text=True)
        if pid_result.returncode == 0:
            pids = pid_result.stdout.strip().split('\n')
            new_pid = pids[-1] if pids else "Unknown"
        else:
            new_pid = "Unknown"

        return f"""## é‡å¯æ•°æ®è·å–æœåŠ¡

**çŠ¶æ€:** âœ… é‡å¯æˆåŠŸ
**æ–°è¿›ç¨‹ID:** {new_pid}
**{stop_info}**

æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## é‡å¯æ•°æ®è·å–æœåŠ¡\nå¤±è´¥: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def get_csv_file_info() -> str:
    """è·å–CSVæ–‡ä»¶ä¿¡æ¯"""
    try:
        if not os.path.exists(CSV_FILE):
            return f"## æ•°æ®æ–‡ä»¶ä¿¡æ¯\næ–‡ä»¶ä¸å­˜åœ¨: {CSV_FILE}"

        with open(CSV_FILE, 'r') as f:
            line_count = sum(1 for _ in f)
        file_size = os.path.getsize(CSV_FILE)
        mtime = os.path.getmtime(CSV_FILE)
        mod_time = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S")

        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        if file_size < 1024:
            size_str = f"{file_size} B"
        elif file_size < 1024 * 1024:
            size_str = f"{file_size / 1024:.2f} KB"
        else:
            size_str = f"{file_size / (1024 * 1024):.2f} MB"

        return f"""## æ•°æ®æ–‡ä»¶ä¿¡æ¯

**æ–‡ä»¶è·¯å¾„:** {CSV_FILE}
**è®°å½•æ•°:** {line_count - 1}æ¡
**æ–‡ä»¶å¤§å°:** {size_str}
**æœ€åä¿®æ”¹:** {mod_time}

æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## æ•°æ®æ–‡ä»¶ä¿¡æ¯\nè¯»å–å¤±è´¥: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


# ==================== å·¥å…·å‡½æ•°ï¼šæ•°æ®åˆ†æ ====================

def get_basic_stats(limit: int = 1000, time_filter: str = None) -> str:
    """è·å–åŸºæœ¬ç»Ÿè®¡æ•°æ®"""
    try:
        if not os.path.exists(CSV_FILE):
            return "## åŸºæœ¬ç»Ÿè®¡\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"

        df_raw = pd.read_csv(CSV_FILE)
        if df_raw.empty:
            return "## åŸºæœ¬ç»Ÿè®¡\nâŒ æ•°æ®æ–‡ä»¶ä¸ºç©º"

        df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])

        if time_filter:
            df_raw = _apply_time_filter_df(df_raw, time_filter)

        if df_raw.empty:
            return f"## åŸºæœ¬ç»Ÿè®¡\nâŒ æ—¶é—´èŒƒå›´ '{time_filter}' å†…æ²¡æœ‰æ•°æ®"

        df_valid = df_raw[df_raw['price'] > 0].copy()
        if df_valid.empty:
            return "## åŸºæœ¬ç»Ÿè®¡\nâŒ æ²¡æœ‰æœ‰æ•ˆäº¤æ˜“æ•°æ®"

        price_min = df_valid['price'].min()
        price_max = df_valid['price'].max()
        price_mean = df_valid['price'].mean()
        price_median = df_valid['price'].median()
        price_std = df_valid['price'].std()

        volume_mean = df_valid['volume'].mean()
        volume_total = df_valid['volume'].sum()

        first_price = df_valid.iloc[0]['price']
        last_price = df_valid.iloc[-1]['price']
        price_change = last_price - first_price
        price_change_pct = (price_change / first_price) * 100

        change_symbol = "+" if price_change >= 0 else ""
        return f"""## åŸºæœ¬ç»Ÿè®¡ç»“æœ

### æ•°æ®æ¦‚å†µ
- åˆ†æè®°å½•æ•°: {len(df_valid)} æ¡
- æ—¶é—´èŒƒå›´: {df_raw['datetime'].min().strftime('%H:%M:%S')} - {df_raw['datetime'].max().strftime('%H:%M:%S')}

### ä»·æ ¼ç»Ÿè®¡
- æœ€ä½ä»·: {round(price_min, 2)}
- æœ€é«˜ä»·: {round(price_max, 2)}
- å¹³å‡ä»·: {round(price_mean, 2)}
- ä¸­ä½æ•°: {round(price_median, 2)}
- ä»·æ ¼æ³¢åŠ¨: {round(price_std, 2)}
- ä»·æ ¼å˜åŒ–: {first_price:.2f} â†’ {last_price:.2f} ({change_symbol}{round(price_change, 2)} / {change_symbol}{round(price_change_pct, 3)}%)

### æˆäº¤é‡ç»Ÿè®¡
- å¹³å‡æˆäº¤é‡: {round(volume_mean, 6)}
- æ€»æˆäº¤é‡: {round(volume_total, 6)}

åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## åŸºæœ¬ç»Ÿè®¡\nâŒ åˆ†æå¤±è´¥: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def analyze_trend(limit: int = 1000, time_filter: str = None) -> str:
    """åˆ†æä»·æ ¼è¶‹åŠ¿"""
    try:
        if not os.path.exists(CSV_FILE):
            return "## è¶‹åŠ¿åˆ†æ\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"

        df_raw = pd.read_csv(CSV_FILE)
        df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])

        if time_filter:
            df_raw = _apply_time_filter_df(df_raw, time_filter)

        if df_raw.empty:
            return f"## è¶‹åŠ¿åˆ†æ\nâŒ æ—¶é—´èŒƒå›´ '{time_filter}' å†…æ²¡æœ‰æ•°æ®"

        df_valid = df_raw[df_raw['price'] > 0].copy()
        if len(df_valid) < 2:
            return "## è¶‹åŠ¿åˆ†æ\nâŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿"

        prices = df_valid['price'].values
        first_price = prices[0]
        last_price = prices[-1]
        total_change = last_price - first_price
        total_change_pct = (total_change / first_price) * 100

        trend = "éœ‡è¡"
        trend_strength = "å¼±"
        if total_change_pct > 0.5:
            trend = "ä¸Šæ¶¨"
            if total_change_pct > 2:
                trend_strength = "å¼º"
            elif total_change_pct > 1:
                trend_strength = "ä¸­"
        elif total_change_pct < -0.5:
            trend = "ä¸‹è·Œ"
            if total_change_pct < -2:
                trend_strength = "å¼º"
            elif total_change_pct < -1:
                trend_strength = "ä¸­"

        up_moves = sum(1 for i in range(1, len(prices)) if prices[i] > prices[i-1])
        down_moves = sum(1 for i in range(1, len(prices)) if prices[i] < prices[i-1])
        up_ratio = (up_moves / (up_moves + down_moves) * 100) if (up_moves + down_moves) > 0 else 50

        change_symbol = "+" if total_change >= 0 else ""
        return f"""## è¶‹åŠ¿åˆ†æç»“æœ

### æ€»ä½“è¶‹åŠ¿
- è¶‹åŠ¿æ–¹å‘: **{trend}** ({trend_strength})
- ä»·æ ¼å˜åŒ–: {round(first_price, 2)} â†’ {round(last_price, 2)} ({change_symbol}{round(total_change, 2)} / {change_symbol}{round(total_change_pct, 3)}%)

### æ³¢åŠ¨åˆ†æ
- ä¸Šæ¶¨æ¬¡æ•°: {up_moves}
- ä¸‹è·Œæ¬¡æ•°: {down_moves}
- ä¸Šæ¶¨å æ¯”: {round(up_ratio, 1)}%

### æ€»ç»“
å…±åˆ†æ {len(df_valid)} æ¡è®°å½•ï¼Œä»·æ ¼å‘ˆ{'ä¸Šå‡' if trend == 'ä¸Šæ¶¨' else 'ä¸‹é™' if trend == 'ä¸‹è·Œ' else 'ä¸­æ€§'}è¶‹åŠ¿ã€‚

åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## è¶‹åŠ¿åˆ†æå¤±è´¥\nâŒ é”™è¯¯: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def analyze_volatility(limit: int = 1000, time_filter: str = None) -> str:
    """åˆ†æä»·æ ¼æ³¢åŠ¨æ€§"""
    try:
        if not os.path.exists(CSV_FILE):
            return "## æ³¢åŠ¨æ€§åˆ†æ\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"

        df_raw = pd.read_csv(CSV_FILE)
        df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])

        if time_filter:
            df_raw = _apply_time_filter_df(df_raw, time_filter)

        if df_raw.empty:
            return f"## æ³¢åŠ¨æ€§åˆ†æ\nâŒ æ—¶é—´èŒƒå›´ '{time_filter}' å†…æ²¡æœ‰æ•°æ®"

        df_valid = df_raw[df_raw['price'] > 0].copy()
        if len(df_valid) < 2:
            return "## æ³¢åŠ¨æ€§åˆ†æ\nâŒ æ•°æ®ä¸è¶³"

        prices = df_valid['price'].values
        mean_price = np.mean(prices)
        std_dev = np.std(prices)
        cv = (std_dev / mean_price) * 100 if mean_price > 0 else 0

        volatility_level = "ä½"
        if cv > 1:
            volatility_level = "æé«˜"
        elif cv > 0.5:
            volatility_level = "é«˜"
        elif cv > 0.2:
            volatility_level = "ä¸­"

        true_range = df_valid['price'].max() - df_valid['price'].min()
        true_range_pct = (true_range / df_valid['price'].min()) * 100

        return f"""## æ³¢åŠ¨æ€§åˆ†æç»“æœ

### æ³¢åŠ¨ç­‰çº§
- æ³¢åŠ¨æ°´å¹³: **{volatility_level}**

### ç»Ÿè®¡æŒ‡æ ‡
- æ ‡å‡†å·®: {round(std_dev, 2)}
- å˜å¼‚ç³»æ•°: {round(cv, 4)}%
- å¹³å‡ä»·æ ¼: {round(mean_price, 2)}

### çœŸå®æ³¢åŠ¨å¹…åº¦
- ç»å¯¹å¹…åº¦: {round(true_range, 2)}
- ç›¸å¯¹å¹…åº¦: {round(true_range_pct, 4)}%

### é£é™©è¯„ä¼°
å½“å‰æ³¢åŠ¨æ€§ç­‰çº§ä¸º **{volatility_level}** {'(é«˜é£é™©)' if volatility_level in ['é«˜', 'æé«˜'] else '(ç›¸å¯¹ç¨³å®š)'}ã€‚

åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## æ³¢åŠ¨æ€§åˆ†æå¤±è´¥\nâŒ é”™è¯¯: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def analyze_time_distribution(limit: int = 10000, time_filter: str = None) -> str:
    """åˆ†ææ—¶é—´åˆ†å¸ƒ"""
    try:
        if not os.path.exists(CSV_FILE):
            return "## æ—¶é—´åˆ†å¸ƒåˆ†æ\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"

        df = pd.read_csv(CSV_FILE)
        df['datetime'] = pd.to_datetime(df['datetime'])

        if time_filter:
            df = _apply_time_filter_df(df, time_filter)

        df_valid = df[df['price'] > 0].copy()
        if df_valid.empty:
            return "## æ—¶é—´åˆ†å¸ƒåˆ†æ\nâŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®"

        df_valid['hour'] = df_valid['datetime'].dt.hour

        hourly_stats = df_valid.groupby('hour').agg({
            'price': ['mean', 'min', 'max', 'count'],
            'volume': 'sum'
        }).round(2)
        hourly_stats.columns = ['avg_price', 'min_price', 'max_price', 'trade_count', 'total_volume']

        most_active = hourly_stats['trade_count'].idxmax()
        least_active = hourly_stats['trade_count'].idxmin()

        output = f"""## æ—¶é—´åˆ†å¸ƒåˆ†æç»“æœ

### æ¦‚è¿°
- è¦†ç›–å°æ—¶æ•°: {len(hourly_stats)}
- æœ€æ´»è·ƒæ—¶æ®µ: {most_active}:00
- æœ€ä¸æ´»è·ƒæ—¶æ®µ: {least_active}:00

### æŒ‰å°æ—¶ç»Ÿè®¡ (å‰5)
| æ—¶æ®µ | å¹³å‡ä»· | æœ€ä½ä»· | æœ€é«˜ä»· | äº¤æ˜“æ¬¡æ•° | æ€»æˆäº¤é‡ |
|------|--------|--------|--------|----------|----------|"""
        top_hours = sorted(hourly_stats.to_dict('records'), key=lambda x: x['trade_count'], reverse=True)[:5]
        for h in top_hours:
            output += f"\n| {int(h['hour'])}:00 | {h['avg_price']:.2f} | {h['min_price']:.2f} | {h['max_price']:.2f} | {int(h['trade_count'])} | {h['total_volume']:.4f} |"

        output += f"\n\nåˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        return output
    except Exception as e:
        return f"## æ—¶é—´åˆ†å¸ƒåˆ†æå¤±è´¥\nâŒ é”™è¯¯: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def compare_time_periods(period1: str = "è¿‡å»ä¸€å°æ—¶", period2: str = "è¿‡å»ä¸€å°æ—¶å‰ä¸€å°æ—¶") -> str:
    """æ¯”è¾ƒä¸¤ä¸ªæ—¶é—´æ®µ"""
    try:
        now = datetime.now()

        def parse_period(p: str):
            p = p.lower().strip()
            if 'è¿‡å»ä¸€å°æ—¶' in p or '1h' in p or '1å°æ—¶' in p:
                return now - timedelta(hours=1), now
            elif 'è¿‡å»ä¸¤å°æ—¶' in p or '2h' in p:
                return now - timedelta(hours=2), now - timedelta(hours=1)
            elif 'ä»Šå¤©' in p or 'ä»Šæ—¥' in p:
                start = now.replace(hour=0, minute=0, second=0)
                return start, now
            elif 'æ˜¨å¤©' in p:
                yesterday = now - timedelta(days=1)
                start = yesterday.replace(hour=0, minute=0, second=0)
                end = yesterday.replace(hour=23, minute=59, second=59)
                return start, end
            return None, None

        start1, end1 = parse_period(period1)
        start2, end2 = parse_period(period2)

        if not start1 or not start2:
            return "## æ—¶é—´æ®µæ¯”è¾ƒ\nâŒ æ— æ³•è§£ææ—¶é—´æ®µ"

        df = pd.read_csv(CSV_FILE)
        df['datetime'] = pd.to_datetime(df['datetime'])

        df1 = df[(df['datetime'] >= start1) & (df['datetime'] <= end1)].copy()
        df2 = df[(df['datetime'] >= start2) & (df['datetime'] <= end2)].copy()
        df1 = df1[df1['price'] > 0]
        df2 = df2[df2['price'] > 0]

        if df1.empty or df2.empty:
            return "## æ—¶é—´æ®µæ¯”è¾ƒ\nâŒ æŸä¸ªæ—¶é—´æ®µæ²¡æœ‰æ•°æ®"

        stats1 = {
            "avg_price": df1['price'].mean(),
            "volume": df1['volume'].sum(),
            "records": len(df1)
        }
        stats2 = {
            "avg_price": df2['price'].mean(),
            "volume": df2['volume'].sum(),
            "records": len(df2)
        }

        avg_change = stats1['avg_price'] - stats2['avg_price']
        avg_change_pct = (avg_change / stats2['avg_price'] * 100) if stats2['avg_price'] > 0 else 0
        vol_change = stats1['volume'] - stats2['volume']
        vol_change_pct = (vol_change / stats2['volume'] * 100) if stats2['volume'] > 0 else 0

        price_sym = "+" if avg_change >= 0 else ""
        vol_sym = "+" if vol_change >= 0 else ""
        return f"""## æ—¶é—´æ®µæ¯”è¾ƒç»“æœ

### {period1}
- å¹³å‡ä»·: {round(stats1['avg_price'], 2)}
- æ€»æˆäº¤é‡: {round(stats1['volume'], 6)}
- è®°å½•æ•°: {stats1['records']}

### {period2}
- å¹³å‡ä»·: {round(stats2['avg_price'], 2)}
- æ€»æˆäº¤é‡: {round(stats2['volume'], 6)}
- è®°å½•æ•°: {stats2['records']}

### å·®å¼‚åˆ†æ
- å¹³å‡ä»·å˜åŒ–: {price_sym}{round(avg_change, 2)} ({price_sym}{round(avg_change_pct, 3)}%)
- æˆäº¤é‡å˜åŒ–: {vol_sym}{round(vol_change, 6)} ({vol_sym}{round(vol_change_pct, 3)}%)

åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## æ—¶é—´æ®µæ¯”è¾ƒå¤±è´¥\nâŒ é”™è¯¯: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def detect_price_anomalies(limit: int = 1000, threshold: float = 2.0, time_filter: str = None) -> str:
    """æ£€æµ‹ä»·æ ¼å¼‚å¸¸"""
    try:
        if not os.path.exists(CSV_FILE):
            return "## ä»·æ ¼å¼‚å¸¸æ£€æµ‹\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"

        df_raw = pd.read_csv(CSV_FILE)
        df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])

        if time_filter:
            df_raw = _apply_time_filter_df(df_raw, time_filter)

        df_valid = df_raw[df_raw['price'] > 0].copy()
        if len(df_valid) < 10:
            return "## ä»·æ ¼å¼‚å¸¸æ£€æµ‹\nâŒ æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘10æ¡è®°å½•"

        prices = df_valid['price'].values
        mean_price = np.mean(prices)
        std_price = np.std(prices)

        upper_bound = mean_price + threshold * std_price
        lower_bound = mean_price - threshold * std_price

        anomalies = df_valid[(df_valid['price'] > upper_bound) | (df_valid['price'] < lower_bound)].copy()

        output = f"""## ä»·æ ¼å¼‚å¸¸æ£€æµ‹ç»“æœ

### æ£€æµ‹å‚æ•°
- é˜ˆå€¼: {threshold}å€æ ‡å‡†å·®
- å¹³å‡ä»·æ ¼: {round(mean_price, 2)}
- æ­£å¸¸èŒƒå›´: {round(lower_bound, 2)} - {round(upper_bound, 2)}

### æ£€æµ‹ç»“æœ
- å‘ç°å¼‚å¸¸ç‚¹: {len(anomalies)} ä¸ª"""

        if len(anomalies) > 0:
            output += "\n### å¼‚å¸¸è¯¦æƒ…\n| æ—¶é—´ | ä»·æ ¼ | åç¦»å€æ•° | æˆäº¤é‡ |\n|------|------|----------|--------|"
            for idx, row in anomalies.head(10).iterrows():
                deviation = (row['price'] - mean_price) / std_price
                output += f"\n| {row['datetime'].strftime('%H:%M:%S')} | {row['price']:.2f} | {deviation:.2f}Ïƒ | {row['volume']:.6f} |"
            if len(anomalies) > 10:
                output += f"\n... è¿˜æœ‰ {len(anomalies) - 10} ä¸ªå¼‚å¸¸ç‚¹"
        else:
            output += "\næœªæ£€æµ‹åˆ°ä»·æ ¼å¼‚å¸¸ï¼Œå¸‚åœºè¡¨ç°ç¨³å®šã€‚"

        output += f"\n\næ£€æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        return output
    except Exception as e:
        return f"## ä»·æ ¼å¼‚å¸¸æ£€æµ‹å¤±è´¥\nâŒ é”™è¯¯: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


# ==================== å·¥å…·å‡½æ•°ï¼šå›¾è¡¨ç”Ÿæˆ ====================

def generate_price_chart(limit: int = 100, time_filter: str = None) -> str:
    """ç”Ÿæˆä»·æ ¼èµ°åŠ¿å›¾"""
    try:
        if not os.path.exists(CSV_FILE):
            return f"## ç”Ÿæˆä»·æ ¼èµ°åŠ¿å›¾\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {CSV_FILE}"

        df = pd.read_csv(CSV_FILE)
        if df.empty:
            return "## ç”Ÿæˆä»·æ ¼èµ°åŠ¿å›¾\nâŒ CSVæ–‡ä»¶ä¸ºç©º"

        df['datetime'] = pd.to_datetime(df['datetime'])

        if time_filter:
            df = _apply_time_filter_df(df, time_filter)
            if df.empty:
                return f"## ç”Ÿæˆä»·æ ¼èµ°åŠ¿å›¾\nâŒ æ—¶é—´èŒƒå›´ '{time_filter}' å†…æ²¡æœ‰æ•°æ®"

        df = df.sort_values('timestamp')
        if not time_filter and len(df) > limit:
            df = df.tail(limit).copy()

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        ax1.plot(df['datetime'], df['price'], label='ä»·æ ¼', color='#2E86AB', linewidth=1.5)
        ax1.set_ylabel('ä»·æ ¼ (USDT)', fontsize=12)
        if time_filter:
            ax1.set_title(f'BTC/USDT ä»·æ ¼èµ°åŠ¿ ({time_filter}, å…±{len(df)}æ¡è®°å½•)', fontsize=14)
        else:
            ax1.set_title(f'BTC/USDT ä»·æ ¼èµ°åŠ¿ (æœ€è¿‘{len(df)}æ¡è®°å½•)', fontsize=14)
        ax1.legend(loc='upper left')
        ax1.grid(True, alpha=0.3)
        ax1.tick_params(axis='x', rotation=45)

        ax2.bar(df['datetime'], df['volume'], label='æˆäº¤é‡', color='#A23B72', alpha=0.6)
        ax2.set_ylabel('æˆäº¤é‡ (BTC)', fontsize=12)
        ax2.set_xlabel('æ—¶é—´', fontsize=12)
        ax2.set_title('æˆäº¤é‡åˆ†å¸ƒ', fontsize=14)
        ax2.legend(loc='upper left')
        ax2.grid(True, alpha=0.3)
        ax2.tick_params(axis='x', rotation=45)

        plt.tight_layout()

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        chart_filename = f"btc_price_chart_{timestamp}.png"
        chart_path = os.path.join(CHART_DIR, chart_filename)
        plt.savefig(chart_path, dpi=100, bbox_inches='tight')
        plt.close()

        from drsai.utils.utils import upload_to_hepai_filesystem
        file_obj = upload_to_hepai_filesystem(chart_path)
        preview_url = file_obj.get("url", "")

        # è®¡ç®—ä»·æ ¼ç»Ÿè®¡
        price_min = df['price'].min()
        price_max = df['price'].max()
        price_avg = df['price'].mean()

        # è¿”å›Markdownæ ¼å¼ï¼Œå¯ç›´æ¥å±•ç¤º
        return f"""## BTCä»·æ ¼èµ°åŠ¿å›¾

![ä»·æ ¼èµ°åŠ¿å›¾]({preview_url})

**æ•°æ®è®°å½•æ•°:** {len(df)}æ¡
**æ—¶é—´èŒƒå›´:** {time_filter if time_filter else 'æœ€è¿‘æ•°æ®'}

### ä»·æ ¼ç»Ÿè®¡
- æœ€é«˜ä»·: {price_min:.2f} USDT
- æœ€ä½ä»·: {price_max:.2f} USDT
- å¹³å‡ä»·: {price_avg:.2f} USDT

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## å›¾è¡¨ç”Ÿæˆå¤±è´¥\né”™è¯¯ä¿¡æ¯: {str(e)}\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def generate_volume_distribution_chart(limit: int = 100, time_filter: str = None) -> str:
    """ç”Ÿæˆæˆäº¤é‡åˆ†å¸ƒå›¾"""
    try:
        if not os.path.exists(CSV_FILE):
            return f"## ç”Ÿæˆæˆäº¤é‡åˆ†å¸ƒå›¾\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {CSV_FILE}"

        df = pd.read_csv(CSV_FILE)
        if df.empty:
            return "## ç”Ÿæˆæˆäº¤é‡åˆ†å¸ƒå›¾\nâŒ CSVæ–‡ä»¶ä¸ºç©º"

        df['datetime'] = pd.to_datetime(df['datetime'])

        if time_filter:
            df = _apply_time_filter_df(df, time_filter)
            if df.empty:
                return f"## ç”Ÿæˆæˆäº¤é‡åˆ†å¸ƒå›¾\nâŒ æ—¶é—´èŒƒå›´ '{time_filter}' å†…æ²¡æœ‰æ•°æ®"

        if not time_filter and len(df) > limit:
            df = df.tail(limit).copy()

        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        ax.hist(df['volume'], bins=30, color='#A23B72', alpha=0.6, edgecolor='black')
        ax.set_xlabel('æˆäº¤é‡ (BTC)', fontsize=12)
        ax.set_ylabel('é¢‘æ¬¡', fontsize=12)
        if time_filter:
            ax.set_title(f'æˆäº¤é‡åˆ†å¸ƒç›´æ–¹å›¾ ({time_filter}, å…±{len(df)}æ¡è®°å½•)', fontsize=14)
        else:
            ax.set_title(f'æˆäº¤é‡åˆ†å¸ƒç›´æ–¹å›¾ (æœ€è¿‘{len(df)}æ¡è®°å½•)', fontsize=14)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()

        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        chart_filename = f"volume_distribution_{timestamp_str}.png"
        chart_path = os.path.join(CHART_DIR, chart_filename)
        plt.savefig(chart_path, dpi=100, bbox_inches='tight')
        plt.close()

        from drsai.utils.utils import upload_to_hepai_filesystem
        file_obj = upload_to_hepai_filesystem(chart_path)
        preview_url = file_obj.get("url", "")

        return f"""## BTCæˆäº¤é‡åˆ†å¸ƒå›¾

![æˆäº¤é‡åˆ†å¸ƒå›¾]({preview_url})

**æ•°æ®è®°å½•æ•°:** {len(df)}æ¡
**æ—¶é—´èŒƒå›´:** {time_filter if time_filter else 'æœ€è¿‘æ•°æ®'}

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## å›¾è¡¨ç”Ÿæˆå¤±è´¥\né”™è¯¯ä¿¡æ¯: {str(e)}\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def get_csv_summary(limit: int = 10, time_filter: str = None) -> str:
    """è·å–CSVæ•°æ®æ‘˜è¦"""
    try:
        if not os.path.exists(CSV_FILE):
            return f"## æ•°æ®æ‘˜è¦\næ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {CSV_FILE}"

        df = pd.read_csv(CSV_FILE)
        if df.empty:
            return "## æ•°æ®æ‘˜è¦\nCSVæ–‡ä»¶ä¸ºç©º"

        df['datetime'] = pd.to_datetime(df['datetime'])

        if time_filter:
            df = _apply_time_filter_df(df, time_filter)
            if df.empty:
                return f"## æ•°æ®æ‘˜è¦\næ—¶é—´èŒƒå›´ '{time_filter}' å†…æ²¡æœ‰æ•°æ®"

        total_records = len(df)

        price_min = df['price'].min()
        price_max = df['price'].max()
        price_avg = df['price'].mean()
        volume_total = df['volume'].sum()

        return f"""## æ•°æ®æ‘˜è¦ç»Ÿè®¡

**æ€»è®°å½•æ•°:** {total_records}æ¡
**æ—¶é—´èŒƒå›´:** {time_filter if time_filter else 'å…¨éƒ¨æ•°æ®'}

### ä»·æ ¼ç»Ÿè®¡
- æœ€ä½ä»·: {price_min:.2f} USDT
- æœ€é«˜ä»·: {price_max:.2f} USDT
- å¹³å‡ä»·: {price_avg:.2f} USDT

### æˆäº¤é‡ç»Ÿè®¡
- æ€»æˆäº¤é‡: {volume_total:.6f} BTC

ç»Ÿè®¡æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except Exception as e:
        return f"## æ•°æ®æ‘˜è¦å¤±è´¥\né”™è¯¯ä¿¡æ¯: {str(e)}\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


# ==================== å·¥å…·å‡½æ•°ï¼šBTCå®æ—¶ä»·æ ¼ ====================

def get_btc_realtime_data() -> str:
    """è·å–BTCå½“å‰ä»·æ ¼ï¼ˆä»æœ¬åœ°CSVæ–‡ä»¶è¯»å–æœ€æ–°æ•°æ®ï¼‰"""
    try:
        if not os.path.exists(CSV_FILE):
            return "## BTCå½“å‰ä»·æ ¼\nâŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆå¯åŠ¨æ•°æ®è·å–æœåŠ¡"

        # è¯»å–CSVæ–‡ä»¶æœ€åä¸€è¡Œ
        with open(CSV_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        if len(lines) <= 1:
            return "## BTCå½“å‰ä»·æ ¼\nâŒ æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œè¯·ç­‰å¾…æ•°æ®è·å–"

        # è§£ææœ€åä¸€è¡Œ (CSVæ ¼å¼: index,datetime,symbol,price,volume,...)
        last_line = lines[-1].strip()
        parts = last_line.split(',')

        if len(parts) < 5:
            return "## BTCå½“å‰ä»·æ ¼\nâŒ æ•°æ®æ ¼å¼é”™è¯¯"

        try:
            # CSVæ ¼å¼: index(0), datetime(1), symbol(2), price(3), volume(4)
            index = parts[0]
            dt_str = parts[1].split('.')[0]  # å»é™¤æ¯«ç§’éƒ¨åˆ†
            symbol = parts[2]
            price = float(parts[3])
            volume = float(parts[4])

            # è®¡ç®—æ•°æ®æ›´æ–°æ—¶é—´å·®
            try:
                from datetime import datetime
                dt = datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")
                time_diff = (datetime.now() - dt).total_seconds()
                time_info = f"ï¼ˆ{int(time_diff)}ç§’å‰æ›´æ–°ï¼‰"
            except:
                time_info = f"ï¼ˆæ›´æ–°æ—¶é—´: {dt_str}ï¼‰"

            return f"""## BTCå½“å‰ä»·æ ¼

**äº¤æ˜“å¯¹:** {symbol}
**å½“å‰ä»·æ ¼:** {price:.2f} USDT
**æˆäº¤é‡:** {volume:.6f} BTC
**æ›´æ–°æ—¶é—´:** {dt_str}

{time_info}

æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        except ValueError as e:
            return f"## BTCå½“å‰ä»·æ ¼\nâŒ æ•°æ®è§£æå¤±è´¥: {str(e)}\n\nåŸå§‹æ•°æ®: {last_line[:100]}"
    except Exception as e:
        return f"## BTCå½“å‰ä»·æ ¼\nâŒ è¯»å–å¤±è´¥: {str(e)}\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


def get_market_status(exchange: str = "US") -> str:
    """è·å–å¸‚åœºçŠ¶æ€"""
    try:
        client = finnhub.Client(api_key="d5ssulhr01qmiccbs4qgd5ssulhr01qmiccbs4r0")
        data = client.market_status(exchange=exchange)
        is_open = data.get('isOpen', False)
        status_text = "ğŸŸ¢ å¼€ç›˜äº¤æ˜“ä¸­" if is_open else "ğŸ”´ å·²æ”¶ç›˜"
        return f"""## å¸‚åœºçŠ¶æ€

**äº¤æ˜“æ‰€:** {exchange}
**çŠ¶æ€:** {status_text}

æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    except:
        return f"## å¸‚åœºçŠ¶æ€\nè·å–å¤±è´¥ï¼šæ— æ³•è·å–å¸‚åœºçŠ¶æ€\n\næŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"


# ==================== åˆ›å»ºæ™ºèƒ½ä½“ ====================

def create_agent(
    thread_id: str = None,
    user_id: str = None,
    db_manager: DatabaseManager = None,
    api_key: str = None
):
    """åˆ›å»ºè‚¡ç¥¨ç›‘æ§æ™ºèƒ½ä½“ï¼ˆå•æ™ºèƒ½ä½“ï¼Œé›†æˆæ‰€æœ‰åŠŸèƒ½ï¼‰"""

    # æ¨¡å‹å®¢æˆ·ç«¯
    model_client = HepAIChatCompletionClient(
        model="deepseek-ai/deepseek-v3",
        api_key=api_key or os.environ.get("HEPAI_API_KEY"),
        base_url="https://aiapi.ihep.ac.cn/apiv2"
    )

    # é›†æˆæ‰€æœ‰å·¥å…·
    all_tools = [
        # === æ•°æ®è·å–ç®¡ç† ===
        check_data_quality,
        get_data_collection_status,
        start_data_collection,
        stop_data_collection,
        restart_data_collection,
        get_csv_file_info,
        # === æ•°æ®åˆ†æ ===
        get_basic_stats,
        analyze_trend,
        analyze_volatility,
        analyze_time_distribution,
        compare_time_periods,
        detect_price_anomalies,
        # === å›¾è¡¨ç”Ÿæˆ ===
        generate_price_chart,
        generate_volume_distribution_chart,
        get_csv_summary,
        # === BTCå®æ—¶ä»·æ ¼ ===
        get_btc_realtime_data,
        get_market_status,
    ]

    return AssistantAgent(
        name="stock_monitor_agent",
        model_client=model_client,
        model_client_stream=True,
        tools=all_tools,
        system_message="""ä½ æ˜¯è‚¡ç¥¨ç›‘æ§ç³»ç»Ÿæ™ºèƒ½åŠ©æ‰‹ï¼Œé›†æˆäº†æ•°æ®è·å–ç®¡ç†ã€æ•°æ®åˆ†æã€å›¾è¡¨ç”Ÿæˆå’ŒBTCå®æ—¶ä»·æ ¼æŸ¥è¯¢åŠŸèƒ½ã€‚

## åŠŸèƒ½åˆ†ç±»

### 1. BTCä»·æ ¼æŸ¥è¯¢
- get_btc_realtime_data: è·å–BTCå®æ—¶ä»·æ ¼
- get_market_status: è·å–å¸‚åœºçŠ¶æ€

### 2. æ•°æ®è·å–ç®¡ç†
- check_data_quality: æ£€æŸ¥æ•°æ®è´¨é‡
- get_data_collection_status: æŸ¥è¯¢æ•°æ®è·å–æœåŠ¡çŠ¶æ€
- start_data_collection: å¯åŠ¨æ•°æ®è·å–æœåŠ¡
- stop_data_collection: åœæ­¢æ•°æ®è·å–æœåŠ¡
- restart_data_collection: é‡å¯æ•°æ®è·å–æœåŠ¡
- get_csv_file_info: è·å–æ•°æ®æ–‡ä»¶ä¿¡æ¯

### 3. æ•°æ®åˆ†æ
- get_basic_stats: è·å–åŸºæœ¬ç»Ÿè®¡ï¼ˆä»·æ ¼ã€æˆäº¤é‡ç»Ÿè®¡ï¼‰
- analyze_trend: åˆ†æä»·æ ¼è¶‹åŠ¿ï¼ˆä¸Šæ¶¨/ä¸‹è·Œ/éœ‡è¡ï¼‰
- analyze_volatility: åˆ†æä»·æ ¼æ³¢åŠ¨æ€§
- analyze_time_distribution: æŒ‰å°æ—¶ç»Ÿè®¡äº¤æ˜“æ´»è·ƒåº¦
- compare_time_periods: æ¯”è¾ƒä¸¤ä¸ªæ—¶é—´æ®µçš„æ•°æ®
- detect_price_anomalies: æ£€æµ‹ä»·æ ¼å¼‚å¸¸

### 4. å›¾è¡¨ç”Ÿæˆ
- generate_price_chart: ç”Ÿæˆä»·æ ¼èµ°åŠ¿å›¾ï¼ˆæ”¯æŒæ—¶é—´è¿‡æ»¤ï¼‰
- generate_volume_distribution_chart: ç”Ÿæˆæˆäº¤é‡åˆ†å¸ƒå›¾
- get_csv_summary: è·å–æ•°æ®æ‘˜è¦ç»Ÿè®¡

## å·¥å…·å‚æ•°è¯´æ˜
- limit: æœ€å¤šåˆ†æNæ¡è®°å½•
- time_filter: æ—¶é—´è¿‡æ»¤è¡¨è¾¾å¼ï¼Œæ”¯æŒï¼š
  * æŒ‰åˆ†é’Ÿ: "30åˆ†é’Ÿ" / "30min"
  * æŒ‰å°æ—¶: "è¿‡å»ä¸€å°æ—¶" / "1h" / "2h" / "6h" / "12h" / "24h"
  * æŒ‰æ—¶æ®µ: "ä»Šå¤©ä¸Šåˆ" / "ä¸Šåˆ" / "ä»Šå¤©ä¸‹åˆ" / "ä¸‹åˆ"
  * æŒ‰å¤©: "ä»Šå¤©" / "ä»Šæ—¥" / "æ˜¨å¤©"
  * å…·ä½“æ—¶é—´æ®µ: "ä¸‹åˆäº”ç‚¹åˆ°å…­ç‚¹" / "17ç‚¹åˆ°18ç‚¹"
- threshold: å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ï¼ˆæ ‡å‡†å·®å€æ•°ï¼‰ï¼Œé»˜è®¤2.0

## å·¥ä½œæµç¨‹
1. ç†è§£ç”¨æˆ·éœ€æ±‚
2. é€‰æ‹©åˆé€‚çš„å·¥å…·
3. è°ƒç”¨å·¥å…·å¹¶è·å–ç»“æœ
4. ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€å‘ç”¨æˆ·å±•ç¤ºç»“æœ   

## é‡è¦æç¤º
- å›¾è¡¨ç”ŸæˆæˆåŠŸåï¼Œä½¿ç”¨Markdownæ ¼å¼å±•ç¤º: ![å›¾è¡¨](preview_url)
- æ‰€æœ‰æŠ¥å‘Šå¿…é¡»åŒ…å«ç³»ç»Ÿæ—¶é—´
- ç”¨ç®€æ´çš„ä¸­æ–‡å›å¤ï¼Œä¸è¦æåŠ"å·¥å…·"ã€"JSON"ã€"è°ƒç”¨"ç­‰æŠ€æœ¯æœ¯è¯­
- æä¾›æ•°æ®é©±åŠ¨çš„æ´å¯Ÿå’Œå»ºè®®
""",
        tool_call_summary_prompt="""è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›å¤ç”¨æˆ·ã€‚

é‡è¦è§„åˆ™ï¼š
1. å¦‚æœå·¥å…·ç»“æœåŒ…å«Markdownæ ¼å¼ï¼ˆå¦‚![å›¾ç‰‡](url)ã€è¡¨æ ¼ã€ç»Ÿè®¡æ•°æ®ç­‰ï¼‰ï¼Œå¿…é¡»**å®Œå…¨ä¿ç•™**åŸå§‹çš„Markdownæ ¼å¼ï¼Œä¸è¦ä¿®æ”¹
2. ç‰¹åˆ«æ˜¯å›¾ç‰‡é“¾æ¥ï¼Œå¿…é¡»åŸæ ·ä¿ç•™ ![å›¾ç‰‡](url) æ ¼å¼
3. åœ¨ä¿ç•™åŸå§‹å†…å®¹çš„åŸºç¡€ä¸Šï¼Œå¯ä»¥åœ¨å¼€å¤´æˆ–ç»“å°¾æ·»åŠ ç®€çŸ­çš„ä¸­æ–‡ç¡®è®¤è¯­å¥
4. ä¸è¦è§£é‡Šæˆ–æ€»ç»“å·¥å…·å·²ç»æ ¼å¼åŒ–å¥½çš„å†…å®¹

ç¤ºä¾‹å›å¤æ ¼å¼ï¼š
"å¥½çš„ï¼Œ[ä»»åŠ¡å·²å®Œæˆ]

[å·¥å…·è¿”å›çš„å®Œæ•´Markdownå†…å®¹ï¼ŒåŸæ ·ä¿ç•™]"
""",
        reflect_on_tool_use=False,
        thread_id=thread_id,
        db_manager=db_manager,
        user_id=user_id,
    )


# ==================== è¿è¡Œæ¨¡å¼ ====================

async def run_console_mode():
    """è¿è¡ŒAgentï¼ˆå‘½ä»¤è¡Œæ¨¡å¼ï¼‰"""
    print("="*60) 
    print("=== è‚¡ç¥¨ç›‘æ§æ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆå‘½ä»¤è¡Œæ¨¡å¼ï¼‰===")
    print("="*60 + "\n")

    task = "æŸ¥çœ‹æ•°æ®è·å–çŠ¶æ€"
    await run_console(agent_factory=create_agent, task=task)


async def run_backend_service():
    """å¯åŠ¨åç«¯APIæœåŠ¡"""
    await run_worker(
        agent_name="stock_monitor_agent",
        author="lin@ihep.ac.cn",
        permission='groups: drsai; users: admin, lin@ihep.ac.cn; owner: lin@ihep.ac.cn',
        description="è‚¡ç¥¨ç›‘æ§æ™ºèƒ½ä½“ç³»ç»Ÿï¼šé›†æˆBTCä»·æ ¼æŸ¥è¯¢ã€æ•°æ®åˆ†æã€å›¾è¡¨ç”Ÿæˆã€æ•°æ®è·å–ç®¡ç†ç­‰åŠŸèƒ½ã€‚",
        version="3.0.0",
        logo="https://aiapi.ihep.ac.cn/apiv2/files/file-8572b27d093f4e15913bebfac3645e20/preview",
        examples=[
            "åˆ†æä»Šå¤©çš„ä»·æ ¼è¶‹åŠ¿",
            "ç”Ÿæˆä»·æ ¼èµ°åŠ¿å›¾",
            "ç”»ä¸€ä¸‹è¿‡å»ä¸€å°æ—¶çš„ä»·æ ¼å›¾è¡¨",
            "åˆ†æä»·æ ¼æ³¢åŠ¨æ€§",
            "å¯åŠ¨æ•°æ®è·å–æœåŠ¡",
            "æŸ¥çœ‹æ•°æ®è·å–çŠ¶æ€",
            "ç”Ÿæˆæˆäº¤é‡åˆ†å¸ƒå›¾",
            "æ¯”è¾ƒä»Šå¤©å’Œæ˜¨å¤©çš„äº¤æ˜“æƒ…å†µ",
            "æ£€æµ‹ä»·æ ¼å¼‚å¸¸æ³¢åŠ¨",
            "æŒ‰å°æ—¶ç»Ÿè®¡äº¤æ˜“æ´»è·ƒåº¦",
            "ç»™æˆ‘ä¸€ä¸ªå…¨é¢çš„æ•°æ®åˆ†ææŠ¥å‘Š",
        ],
        agent_factory=create_agent,
        host="0.0.0.0",
        port=42820,
        no_register=False,
        enable_openwebui_pipeline=True,
        history_mode="backend",
        use_api_key_mode="backend",
    )


if __name__ == "__main__":
    if DEBUG_MODE:
        asyncio.run(run_console_mode())
    else:
        asyncio.run(run_backend_service())
